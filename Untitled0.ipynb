{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+OHFPsrX9yRz2H0CSBZCf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manzoor-22/AI/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Stop Words\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "str1 = \"Remove stop words from this sentence using NLTK.\"\n",
        "str2 = \"NLTK is powerful tool for natural language processing.\"\n",
        "str3 = \"Stop words are common words that are often excluded in text analysis.\"\n",
        "\n",
        "def stop_words_removal(text):\n",
        "  words = word_tokenize(text)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  print(\"Original text: \", text)\n",
        "  filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "  filtered_text = ' '.join(filtered_words)\n",
        "  print(\"Filtered Text: \", filtered_text)\n",
        "\n",
        "stop_words_removal(str1)\n",
        "stop_words_removal(str2)\n",
        "stop_words_removal(str3)"
      ],
      "metadata": {
        "id": "88un_3ZtTWHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b5a5f5-b92d-4ddd-dde3-1491ba033ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  Remove stop words from this sentence using NLTK.\n",
            "Filtered Text:  Remove stop words sentence using NLTK .\n",
            "Original text:  NLTK is powerful tool for natural language processing.\n",
            "Filtered Text:  NLTK powerful tool natural language processing .\n",
            "Original text:  Stop words are common words that are often excluded in text analysis.\n",
            "Filtered Text:  Stop words common words often excluded text analysis .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "text = \"changing changed change\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Text: \" + text)\n",
        "print(\"Stemmed Text: \", stemmed_words)\n"
      ],
      "metadata": {
        "id": "lkTdL_IBUlkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6ffac8-bb16-4df8-dad7-a261d16a872b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: changing changed change\n",
            "Stemmed Text:  ['chang', 'chang', 'chang']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS Tagging\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"I am studing in MJCET, Hyderabad\"\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "tagged_words = pos_tag(words)\n",
        "\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCwCiM_JAKhO",
        "outputId": "27d3d675-b32f-4a06-a312-64e7fc4f3a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('studing', 'VBG'), ('in', 'IN'), ('MJCET', 'NNP'), (',', ','), ('Hyderabad', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "\n",
        "graph = {'A' : {'B' : 5, 'C' : 3},\n",
        "        'B' : {'D' : 8, 'E': 6},\n",
        "        'C' : {'E' : 2, 'F' : 4},\n",
        "        'D' : {'G' : 9},\n",
        "        'E' : {'G' : 7},\n",
        "        'F' : {},\n",
        "        'G' : {}\n",
        "}\n",
        "\n",
        "heuristic = {\n",
        "    'A' : 10,\n",
        "    'B' : 8,\n",
        "    'C' : 7,\n",
        "    'D' : 6,\n",
        "    'E' : 4,\n",
        "    'F' : 3,\n",
        "    'G' : 0\n",
        "}\n",
        "\n",
        "def gbfs(graph, start, goal, heuristic):\n",
        "  visited = set()\n",
        "  priority_queue = [(heuristic[start], start)]\n",
        "  came_from = {}\n",
        "\n",
        "  while priority_queue:\n",
        "    current_cost, current_node = heapq.heappop(priority_queue)\n",
        "\n",
        "    if current_node in visited:\n",
        "      continue\n",
        "\n",
        "    visited.add(current_node)\n",
        "\n",
        "    if current_node == goal:\n",
        "      path = [current_node]\n",
        "      while current_node in came_from:\n",
        "        current_node = came_from[current_node]\n",
        "        path.insert(0, current_node)\n",
        "      return path\n",
        "\n",
        "    for neighbour, cost in graph[current_node].items():\n",
        "      if neighbour not in visited:\n",
        "        heapq.heappush(priority_queue, (heuristic[neighbour], neighbour))\n",
        "        came_from[neighbour] = current_node\n",
        "\n",
        "  return None\n",
        "\n",
        "print(gbfs(graph, 'A', 'G', heuristic))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW6jYcZMD2GB",
        "outputId": "e58ac3d6-8af7-4c8f-c52f-ed8cb4db7417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'C', 'E', 'G']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bX-q-uYNMdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}